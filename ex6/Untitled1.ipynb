{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# ### Purpose of the notebook:\n",
    "# In this notebook, you'll see a simple demo of implementing Principle Component Analysis -- a dimensionality reduction method that works by creating a projection on a smaller dimension which minimizes the distance between actual features and the new projection. It is a popular method for visualization, feature generation and unsupervised clustering.\n",
    "# \n",
    "# \n",
    "\n",
    "# ### 1. Import packages:\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "matplotlib.style.use('ggplot')\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# ### 2. Generate data:\n",
    "# For this first exercise, we will start with simulated data defined by a 2-dimensional Normal (Gaussian) distribution with mean vector (0,0) and covariance matrix [[3,1],[1,2]]. \n",
    "\n",
    "# In[148]:\n",
    "\n",
    "# Define mean vector and covariance matrix:\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "\n",
    "mu = np.zeros(2)\n",
    "\n",
    "cov = np.array([[3,1],[1,2]])\n",
    "\n",
    "sample=1000\n",
    "data = np.random.multivariate_normal(mu,cov,size=sample)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "plt.xlabel('X1')\n",
    "plt.ylabel('X2')\n",
    "plt.title('Simulated data (2-dimensional Gaussian)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ### 3. Implement PCA by Eigenvalue decomposition:\n",
    "# \n",
    "# Essentially, PCA provides a transformed set of features such that the transformed numbers and the original features are as close as possible. One way to do that is to apply an eigen-value decomposition (which is equivalent in this case to a SVD) on the covariance matrix of a data set, so that all variables are transformed to be othogonal of each other.\n",
    "# \n",
    "# Since this simulated data already comes with a covariance matrix, we can use `np.linalg.eig()` to get the eigenvalues and eigenvectors and use those to transform our original data into PC's\n",
    "# \n",
    "# \n",
    "\n",
    "# In[150]:\n",
    "\n",
    "# eigen-value decomposition of the covariance matrix\n",
    "[S,W_true]=np.linalg.eig(cov)\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "\n",
    "plt.scatter(data[:,0],data[:,1])\n",
    "# Define transformed features:\n",
    "z1=(W_true[0,0]/W_true[0,1])*data[:,0]\n",
    "z2=(W_true[1,0]/W_true[1,1])*data[:,0]\n",
    "z=np.array([z1,z2]).transpose()\n",
    "\n",
    "plt.plot(data[:,0], z1,color='g')\n",
    "plt.plot(data[:,0], z2,color='g')\n",
    "\n",
    "g_patch = mpatches.Patch(color='g', label='Eigen-vectors (true PC components)')\n",
    "\n",
    "plt.legend(handles=[g_patch])\n",
    "plt.axis('equal')\n",
    "\n",
    "limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1])),\n",
    "          np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))]\n",
    "\n",
    "plt.xlim(limits[0],limits[1])\n",
    "plt.ylim(limits[0],limits[1])\n",
    "plt.draw()\n",
    "\n",
    "\n",
    "# ### 4. Define function to plot principal components\n",
    "\n",
    "# In[183]:\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_pc(data, pca, scatter=True, legend=True,g_patch=None):\n",
    "    #fig=plt.figure(figsize=(8,6))\n",
    "    # Extract weight matrix from PCA model\n",
    "    W_pca = pca.components_\n",
    "    if scatter:\n",
    "        \n",
    "        plt.scatter(data[:,0], data[:,1])\n",
    "        \n",
    "    plt.plot(data[:,0], -(W_pca[0,0]/W_pca[0,1])*data[:,0], color=\"c\")\n",
    "    \n",
    "    plt.plot(data[:,0], -(W_pca[1,0]/W_pca[1,1])*data[:,0], color=\"c\")\n",
    "    \n",
    "    if legend:\n",
    "        \n",
    "        c_patch = mpatches.Patch(color='c', label='Principal components')\n",
    "        \n",
    "        plt.legend(handles=[c_patch,g_patch], loc='lower right')\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    \n",
    "    \n",
    "    limits = [np.minimum(np.amin(data[:,0]), np.amin(data[:,1]))-0.5,\n",
    "              np.maximum(np.amax(data[:,0]), np.amax(data[:,1]))+0.5]\n",
    "    \n",
    "    plt.xlim(limits[0],limits[1])\n",
    "    plt.ylim(limits[0],limits[1])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ### 5. Compare results using PCA vs using eigen-value decomposition:\n",
    "\n",
    "# In[184]:\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1])\n",
    "\n",
    "plt.plot(data[:,0], z1, color=\"g\")\n",
    "plt.plot(data[:,0], z2, color=\"g\")\n",
    "\n",
    "plot_pc(data, pca, scatter=True, legend=True,g_patch=g_patch)\n",
    "\n",
    "\n",
    "# ### Exercise: try PCA with the Iris dataset\n",
    "# \n",
    "# sklearn has some built-in data sets, let's play around with the Iris dataset, which consists of 3 different types of irisesâ€™ (Setosa, Versicolour, and Virginica) petal and sepal length, stored in a 150x4 numpy.ndarray\n",
    "# \n",
    "# The rows are the samples and the columns are: Sepal Length, Sepal Width, Petal Length and Petal Width, so this is a 4-dimensional data set with more features than we can visualize by eye.\n",
    "\n",
    "# In[209]:\n",
    "\n",
    "## Load iris data:\n",
    "iris = datasets.load_iris()\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "target_names = iris.target_names\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "\n",
    "xdata = data[:,0]\n",
    "ydata = data[:,1]\n",
    "zdata = data[:,2]\n",
    "\n",
    "ax.scatter3D(xdata, ydata, zdata, c=target,cmap=plt.cm.RdYlGn,edgecolor='k')\n",
    "\n",
    "\n",
    "ax.set_xlabel('Sepal Length')\n",
    "\n",
    "ax.set_ylabel('Sepal Width')\n",
    "\n",
    "ax.set_zlabel('Petal Length')\n",
    "\n",
    "ax.set_title('Visualizing Sepal Length, Sepal Width and Petal Length')\n",
    "\n",
    "\n",
    "# In[189]:\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "def plot_iris(transformed_data, target, target_names):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ## or each target, plot the data with corresponding target label and color (r, g, or b)\n",
    "    for c, i, target_name in zip(\"ryg\", [0, 1, 2], target_names):\n",
    "        plt.scatter(transformed_data[target == i, 0],\n",
    "                    transformed_data[target == i, 1], c=c, label=target_name)\n",
    "    plt.xlabel('PC 1')\n",
    "    plt.ylabel('PC 2')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def write_answer(list_pc1, list_pc2):\n",
    "    with open(\"pca_answer3.txt\", \"w\") as fout:\n",
    "        fout.write(\" \".join([str(num) for num in list_pc1]))\n",
    "        fout.write(\" \")\n",
    "        fout.write(\" \".join([str(num) for num in list_pc2]))\n",
    "\n",
    "\n",
    "# ### Dimensionality reduction:\n",
    "# \n",
    "# Using PCA, we can reduce the dimension of our feature space from 4 to 2 without losing too much information. Below is a demo:\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "# Create a PCA instance 2 dimensions:\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA instance with the iris data: \n",
    "\n",
    "princomp= pca.fit_transform(data)\n",
    "\n",
    "plot_iris(transformed_data=princomp,target=target,target_names=target_names)\n",
    "\n",
    "\n",
    "# ### Let's take a look at the principle components:\n",
    "# \n",
    "# sklearn's PCA has an attribute called `explained_variance_ratio_` that tells us how much of the data's variation is explained by the chosen PC's. Essentially, that is the threshold we can use to measure against the projection error between original features and transformed features\n",
    "\n",
    "# In[253]:\n",
    "\n",
    "\n",
    "sum(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "# In[247]:\n",
    "\n",
    "def iris_label(x):\n",
    "    if x == 0:\n",
    "        lab='setosa'\n",
    "    elif x == 1:\n",
    "        lab = 'versicolor'\n",
    "    elif x == 2:\n",
    "        lab = 'virginica'\n",
    "    else:\n",
    "        lab = 'other'\n",
    "    return lab\n",
    "\n",
    "princomp_df=pd.DataFrame(data=princomp,columns=['PC1','PC2'])\n",
    "\n",
    "iris_df=pd.DataFrame(data=iris.data)\n",
    "iris_df.columns=['Sepal Length','Sepal Width','Petal Length','Petal Width']\n",
    "iris_target=pd.DataFrame(data=iris.target)\n",
    "iris_df['Label']=iris_target\n",
    "\n",
    "iris_df['Label']=iris_df['Label'].apply(lambda s : iris_label(s))\n",
    "\n",
    "\n",
    "# In[248]:\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(iris_df,hue='Label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
